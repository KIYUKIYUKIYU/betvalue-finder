#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
å¤±æ•—ãƒ­ã‚°ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰æœ€é©ãªãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œã‚’ç”Ÿæˆ
"""

import sys
from pathlib import Path
from typing import Dict, List
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from converter.auto_transliterator import AutoTransliterator
from tools.confidence_scorer import ConfidenceScorer


class MappingCandidateGenerator:
    """ãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œã‚’ç”Ÿæˆã—ã€ä¿¡é ¼åº¦ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.transliterator = AutoTransliterator()
        self.scorer = ConfidenceScorer()
        self.unified_teams_path = Path(__file__).parent.parent / "database" / "unified_teams.json"
        self.existing_mappings = self._load_existing_mappings()

    def _load_existing_mappings(self) -> Dict:
        """æ—¢å­˜ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.unified_teams_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def generate_candidates(self, failure_report: Dict) -> Dict:
        """
        å¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰å€™è£œã‚’ç”Ÿæˆ

        Args:
            failure_report: analyze_failure_log.pyã®å‡ºåŠ›

        Returns:
            {
                "api_team": {
                    "candidates": [
                        {
                            "japanese": "ã‚«ã‚¿ã‚«ãƒŠå",
                            "source": "user_input" | "transliteration" | "pattern",
                            "confidence": 85.5,
                            "reasoning": {...}
                        }
                    ],
                    "recommended_candidate": {...},  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å€™è£œ
                    "api_occurrences": 5,
                    "failure_types": {...}
                }
            }
        """
        results = {}

        for api_team, data in failure_report.items():
            candidates = []

            # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ™ãƒ¼ã‚¹å€™è£œ
            user_candidates = self._generate_user_input_candidates(
                api_team, data
            )
            candidates.extend(user_candidates)

            # 2. éŸ³è¨³ãƒ™ãƒ¼ã‚¹å€™è£œ
            transliteration_candidates = self._generate_transliteration_candidates(
                api_team, data
            )
            candidates.extend(transliteration_candidates)

            # 3. æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹å€™è£œ
            pattern_candidates = self._generate_pattern_candidates(
                api_team, data
            )
            candidates.extend(pattern_candidates)

            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            candidates.sort(key=lambda x: x["confidence"], reverse=True)

            # æ¨å¥¨å€™è£œï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
            recommended = candidates[0] if candidates else None

            results[api_team] = {
                "candidates": candidates,
                "recommended_candidate": recommended,
                "api_occurrences": data.get("api_occurrences", 0),
                "failure_types": data.get("failure_types", {})
            }

        return results

    def _generate_user_input_candidates(
        self, api_team: str, data: Dict
    ) -> List[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰å€™è£œã‚’ç”Ÿæˆ"""
        candidates = []
        user_inputs = data.get("user_inputs", {})

        for japanese, frequency in user_inputs.items():
            if not japanese:
                continue

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            score = self.scorer.score_mapping(
                api_team,
                japanese,
                {
                    "api_frequency": data.get("api_occurrences", 0),
                    "user_input_frequency": frequency,
                    "similar_existing_mappings": self._count_similar_patterns(api_team),
                    "has_official_translation": False
                }
            )

            candidates.append({
                "japanese": japanese,
                "source": "user_input",
                "confidence": score,
                "reasoning": {
                    "user_frequency": frequency,
                    "api_frequency": data.get("api_occurrences", 0),
                    "similar_patterns": self._count_similar_patterns(api_team)
                }
            })

        return candidates

    def _generate_transliteration_candidates(
        self, api_team: str, data: Dict
    ) -> List[Dict]:
        """éŸ³è¨³ã‹ã‚‰å€™è£œã‚’ç”Ÿæˆ"""
        candidates = []

        try:
            japanese_options = self.transliterator.generate_japanese_candidates(api_team)

            for japanese in japanese_options:
                # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                score = self.scorer.score_mapping(
                    api_team,
                    japanese,
                    {
                        "api_frequency": data.get("api_occurrences", 0),
                        "user_input_frequency": 0,  # éŸ³è¨³ãªã®ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãªã—
                        "similar_existing_mappings": self._count_similar_patterns(api_team),
                        "has_official_translation": False
                    }
                )

                candidates.append({
                    "japanese": japanese,
                    "source": "transliteration",
                    "confidence": score,
                    "reasoning": {
                        "transliteration_method": "auto",
                        "api_frequency": data.get("api_occurrences", 0)
                    }
                })

        except Exception as e:
            # éŸ³è¨³å¤±æ•—æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
            pass

        return candidates

    def _generate_pattern_candidates(
        self, api_team: str, data: Dict
    ) -> List[Dict]:
        """æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å€™è£œã‚’ç”Ÿæˆ"""
        candidates = []

        # é¡ä¼¼ãƒãƒ¼ãƒ ã‚’æ¤œç´¢
        similar_teams = self._find_similar_teams(api_team)

        for similar_english, similar_japanese_list in similar_teams[:3]:  # Top 3
            # æœ€åˆã®æ—¥æœ¬èªå€™è£œã‚’ä½¿ç”¨
            if similar_japanese_list:
                japanese = similar_japanese_list[0]

                # api_teamã«é©ç”¨ï¼ˆä¾‹: Manchester City â†’ Manchester United ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æµç”¨ï¼‰
                # ç°¡æ˜“å®Ÿè£…: ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šé«˜åº¦ãªå¤‰æ›ãŒå¿…è¦ï¼‰

                score = self.scorer.score_mapping(
                    api_team,
                    japanese,
                    {
                        "api_frequency": data.get("api_occurrences", 0),
                        "user_input_frequency": 0,
                        "similar_existing_mappings": len(similar_teams),
                        "has_official_translation": False
                    }
                )

                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã¯ã‚¹ã‚³ã‚¢ã‚’å°‘ã—ä¸‹ã’ã‚‹ï¼ˆä¸ç¢ºå®Ÿæ€§ï¼‰
                score *= 0.8

                candidates.append({
                    "japanese": japanese,
                    "source": "pattern",
                    "confidence": score,
                    "reasoning": {
                        "similar_team": similar_english,
                        "pattern_count": len(similar_teams)
                    }
                })

        return candidates

    def _count_similar_patterns(self, api_team: str) -> int:
        """é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = 0
        api_lower = api_team.lower()
        words = api_lower.split()

        for existing_team in self.existing_mappings.keys():
            existing_lower = existing_team.lower()

            # å˜èªã®é‡è¤‡ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_words = set(existing_lower.split())
            api_words = set(words)
            overlap = len(api_words & existing_words)

            if overlap > 0:
                count += 1

        return min(count, 10)  # æœ€å¤§10

    def _find_similar_teams(self, api_team: str) -> List[tuple]:
        """é¡ä¼¼ãƒãƒ¼ãƒ ã‚’æ¤œç´¢"""
        from difflib import SequenceMatcher

        similarities = []
        api_lower = api_team.lower()

        for existing_team, japanese_list in self.existing_mappings.items():
            existing_lower = existing_team.lower()

            # æ–‡å­—åˆ—é¡ä¼¼åº¦
            ratio = SequenceMatcher(None, api_lower, existing_lower).ratio()

            if ratio > 0.3:  # 30%ä»¥ä¸Šã®é¡ä¼¼åº¦
                similarities.append((existing_team, japanese_list, ratio))

        # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        similarities.sort(key=lambda x: x[2], reverse=True)

        return [(team, jp_list) for team, jp_list, _ in similarities]

    def print_candidates(self, results: Dict, top_n: int = 5):
        """å€™è£œã‚’è¦‹ã‚„ã™ãè¡¨ç¤º"""
        print("\n" + "=" * 80)
        print(f"ğŸ’¡ ãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œç”Ÿæˆçµæœ (Top {top_n})")
        print("=" * 80)

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            results.items(),
            key=lambda x: x[1]["recommended_candidate"]["confidence"] if x[1]["recommended_candidate"] else 0,
            reverse=True
        )

        for i, (api_team, data) in enumerate(sorted_results[:top_n], 1):
            recommended = data["recommended_candidate"]

            if not recommended:
                continue

            confidence = recommended["confidence"]
            japanese = recommended["japanese"]
            source = recommended["source"]

            # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹çµµæ–‡å­—
            if confidence >= 90:
                emoji = "âœ…"
                level = "é«˜"
            elif confidence >= 70:
                emoji = "ğŸŸ¡"
                level = "ä¸­"
            else:
                emoji = "âš ï¸ "
                level = "ä½"

            print(f"\n{i}. {emoji} {api_team} â†’ ã€Œ{japanese}ã€")
            print(f"   ä¿¡é ¼åº¦: {confidence:.1f}ç‚¹ ({level}ä¿¡é ¼åº¦)")
            print(f"   å‡ºå…¸: {source}")
            print(f"   APIå‡ºç¾: {data['api_occurrences']}å›")

            # ä»–ã®å€™è£œ
            other_candidates = [c for c in data["candidates"] if c != recommended]
            if other_candidates and len(other_candidates) > 0:
                print(f"   ãã®ä»–å€™è£œ:")
                for cand in other_candidates[:2]:  # ä¸Šä½2ä»¶
                    print(f"     - ã€Œ{cand['japanese']}ã€ ({cand['confidence']:.1f}ç‚¹, {cand['source']})")

        print("\n" + "=" * 80)


def main():
    """CLIã¨ã—ã¦å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="ãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³")
    parser.add_argument(
        "--report",
        required=True,
        help="å¤±æ•—ãƒ­ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSONï¼‰"
    )
    parser.add_argument(
        "--output",
        help="å€™è£œãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›å…ˆï¼ˆJSONï¼‰"
    )
    parser.add_argument(
        "--top",
        type=int,
        default=10,
        help="è¡¨ç¤ºã™ã‚‹ä¸Šä½Nä»¶"
    )

    args = parser.parse_args()

    # ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    with open(args.report, "r", encoding="utf-8") as f:
        failure_report = json.load(f)

    print(f"ğŸ” ãƒãƒƒãƒ”ãƒ³ã‚°å€™è£œç”Ÿæˆé–‹å§‹")
    print(f"   å¯¾è±¡: {len(failure_report)}ãƒãƒ¼ãƒ ")

    generator = MappingCandidateGenerator()
    results = generator.generate_candidates(failure_report)

    generator.print_candidates(results, top_n=args.top)

    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ å€™è£œãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")


if __name__ == "__main__":
    main()
