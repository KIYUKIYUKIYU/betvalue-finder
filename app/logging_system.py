# -*- coding: utf-8 -*-
import logging
import logging.handlers
import json
import traceback
from datetime import datetime
from typing import Dict, Any, Optional
from contextlib import contextmanager
import time
import psutil
import threading
from pathlib import Path
import os

# ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«å®šç¾©
PIPELINE_SUCCESS = 25  # INFO ã¨ WARNING ã®é–“
BUSINESS_WARNING = 35  # WARNING ã¨ ERROR ã®é–“

logging.addLevelName(PIPELINE_SUCCESS, "PIPELINE_SUCCESS")
logging.addLevelName(BUSINESS_WARNING, "BUSINESS_WARNING")

class BetValueLogManager:
    def __init__(self,
                 log_dir: str = "logs",
                 max_file_size: int = 10*1024*1024,  # 10MB
                 backup_count: int = 5,
                 enable_console: bool = True,
                 enable_file: bool = True,
                 enable_structured: bool = True):

        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        # ãƒ­ã‚°è¨­å®š
        self.setup_loggers(max_file_size, backup_count, enable_console,
                          enable_file, enable_structured)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
        self.metrics = {
            'requests': 0,
            'errors': 0,
            'pipeline_successes': 0,
            'pipeline_failures': 0,
            'avg_response_time': 0.0,
            'system_health': {},
            'startup_time': datetime.utcnow().isoformat()
        }

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._system_monitoring, daemon=True)
        self.monitoring_thread.start()

        # åˆæœŸåŒ–ãƒ­ã‚°
        self.main_logger.info("ğŸš€ BetValue Finder Logging System initialized")
        self.main_logger.info(f"ğŸ“ Log directory: {self.log_dir.absolute()}")

    def setup_loggers(self, max_file_size, backup_count, enable_console, enable_file, enable_structured):
        # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¬ãƒ¼
        self.main_logger = logging.getLogger('betvalue.main')
        self.main_logger.setLevel(logging.DEBUG)

        # Pipelineå°‚ç”¨ãƒ­ã‚¬ãƒ¼
        self.pipeline_logger = logging.getLogger('betvalue.pipeline')
        self.pipeline_logger.setLevel(logging.DEBUG)

        # APIå°‚ç”¨ãƒ­ã‚¬ãƒ¼
        self.api_logger = logging.getLogger('betvalue.api')
        self.api_logger.setLevel(logging.DEBUG)

        # ã‚¨ãƒ©ãƒ¼å°‚ç”¨ãƒ­ã‚¬ãƒ¼
        self.error_logger = logging.getLogger('betvalue.errors')
        self.error_logger.setLevel(logging.WARNING)

        # æ—¢å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        for logger in [self.main_logger, self.pipeline_logger, self.api_logger, self.error_logger]:
            logger.handlers.clear()

        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        detailed_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s:%(lineno)d | %(message)s'
        )

        json_formatter = self.JSONFormatter()

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        if enable_console:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(detailed_formatter)
            console_handler.setLevel(logging.INFO)  # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã¯ INFO ä»¥ä¸Šã®ã¿
            self.main_logger.addHandler(console_handler)

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        if enable_file:
            # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            main_file_handler = logging.handlers.RotatingFileHandler(
                self.log_dir / 'betvalue_main.log',
                maxBytes=max_file_size,
                backupCount=backup_count,
                encoding='utf-8'
            )
            main_file_handler.setFormatter(detailed_formatter)
            self.main_logger.addHandler(main_file_handler)

            # Pipelineå°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«
            pipeline_file_handler = logging.handlers.RotatingFileHandler(
                self.log_dir / 'pipeline.log',
                maxBytes=max_file_size,
                backupCount=backup_count,
                encoding='utf-8'
            )
            pipeline_file_handler.setFormatter(detailed_formatter)
            self.pipeline_logger.addHandler(pipeline_file_handler)

            # ã‚¨ãƒ©ãƒ¼å°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«
            error_file_handler = logging.handlers.RotatingFileHandler(
                self.log_dir / 'errors.log',
                maxBytes=max_file_size,
                backupCount=backup_count,
                encoding='utf-8'
            )
            error_file_handler.setFormatter(detailed_formatter)
            self.error_logger.addHandler(error_file_handler)

        # æ§‹é€ åŒ–JSONãƒ­ã‚°
        if enable_structured:
            json_handler = logging.handlers.RotatingFileHandler(
                self.log_dir / 'structured.jsonl',
                maxBytes=max_file_size,
                backupCount=backup_count,
                encoding='utf-8'
            )
            json_handler.setFormatter(json_formatter)

            # å…¨ãƒ­ã‚¬ãƒ¼ã«æ§‹é€ åŒ–ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
            for logger in [self.main_logger, self.pipeline_logger, self.api_logger, self.error_logger]:
                logger.addHandler(json_handler)

    class JSONFormatter(logging.Formatter):
        def format(self, record):
            log_entry = {
                'timestamp': datetime.utcnow().isoformat(),
                'level': record.levelname,
                'logger': record.name,
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno,
                'message': record.getMessage(),
                'thread': record.thread,
                'process': record.process
            }

            # è¿½åŠ ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            if hasattr(record, 'extra_data'):
                log_entry.update(record.extra_data)

            # ä¾‹å¤–æƒ…å ±
            if record.exc_info and record.exc_info[0] is not None:
                log_entry['exception'] = {
                    'type': record.exc_info[0].__name__,
                    'message': str(record.exc_info[1]),
                    'traceback': traceback.format_exception(*record.exc_info)
                }

            return json.dumps(log_entry, ensure_ascii=False)

    @contextmanager
    def log_performance(self, operation_name: str, logger_name: str = 'main'):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šä»˜ããƒ­ã‚°"""
        logger = getattr(self, f'{logger_name}_logger')
        start_time = time.time()

        logger.info(f"ğŸš€ Starting {operation_name}")

        try:
            yield
            elapsed = time.time() - start_time
            logger.info(f"âœ… Completed {operation_name} in {elapsed:.3f}s")

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"âŒ Failed {operation_name} after {elapsed:.3f}s: {str(e)}")
            self.log_error(f"Performance tracking error in {operation_name}", e)
            raise

    def log_request(self, request_info: Dict[str, Any]):
        """APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ­ã‚°"""
        extra_data = {
            'event_type': 'api_request',
            'request_id': request_info.get('request_id'),
            'method': request_info.get('method'),
            'path': request_info.get('path'),
            'query_params': request_info.get('query_params'),
            'user_agent': request_info.get('user_agent'),
            'ip_address': request_info.get('ip_address'),
            'processing_time': request_info.get('processing_time')
        }

        status_code = request_info.get('status_code', 0)
        processing_time = request_info.get('processing_time', 0)

        self.api_logger.info(
            f"API {request_info['method']} {request_info['path']} - "
            f"{status_code} ({processing_time:.3f}s)",
            extra={'extra_data': extra_data}
        )

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        self.metrics['requests'] += 1
        if status_code >= 400:
            self.metrics['errors'] += 1

        # å¹³å‡å¿œç­”æ™‚é–“æ›´æ–°
        if self.metrics['avg_response_time'] == 0:
            self.metrics['avg_response_time'] = processing_time
        else:
            self.metrics['avg_response_time'] = (
                (self.metrics['avg_response_time'] * (self.metrics['requests'] - 1) + processing_time) /
                self.metrics['requests']
            )

    def log_pipeline_stage(self, stage_info: Dict[str, Any]):
        """Pipelineæ®µéšãƒ­ã‚°"""
        extra_data = {
            'event_type': 'pipeline_stage',
            'stage_name': stage_info.get('stage_name'),
            'success': stage_info.get('success'),
            'processing_time': stage_info.get('processing_time'),
            'input_data': stage_info.get('input_summary'),
            'output_data': stage_info.get('output_summary'),
            'quality_metrics': stage_info.get('quality_metrics'),
            'error_message': stage_info.get('error_message')
        }

        stage_name = stage_info.get('stage_name', 'Unknown')
        processing_time = stage_info.get('processing_time', 0)

        if stage_info.get('success'):
            self.pipeline_logger.log(
                PIPELINE_SUCCESS,
                f"âœ… {stage_name} completed successfully ({processing_time:.3f}s)",
                extra={'extra_data': extra_data}
            )
            self.metrics['pipeline_successes'] += 1
        else:
            error_msg = stage_info.get('error_message', 'Unknown error')
            self.pipeline_logger.error(
                f"âŒ {stage_name} failed: {error_msg}",
                extra={'extra_data': extra_data}
            )
            self.metrics['pipeline_failures'] += 1

    def log_error(self, message: str, error: Exception,
                  context: Optional[Dict[str, Any]] = None):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ä»˜ãï¼‰"""
        extra_data = {
            'event_type': 'error',
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context or {}
        }

        self.error_logger.error(
            f"ğŸš¨ {message}: {str(error)}",
            exc_info=True,
            extra={'extra_data': extra_data}
        )

    def log_business_event(self, event_type: str, details: Dict[str, Any]):
        """ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯é–¢é€£ã®ãƒ­ã‚°"""
        extra_data = {
            'event_type': 'business',
            'business_event': event_type,
            'details': details
        }

        if event_type in ['low_confidence_mapping', 'unusual_odds', 'api_limit_warning']:
            self.main_logger.log(
                BUSINESS_WARNING,
                f"âš ï¸ Business Warning: {event_type}",
                extra={'extra_data': extra_data}
            )
        else:
            self.main_logger.info(
                f"ğŸ“Š Business Event: {event_type}",
                extra={'extra_data': extra_data}
            )

    def pipeline_success(self, message, extra=None):
        """PipelineæˆåŠŸãƒ­ã‚°"""
        self.pipeline_logger.log(PIPELINE_SUCCESS, message, extra=extra)

    def business_warning(self, message, extra=None):
        """ãƒ“ã‚¸ãƒã‚¹è­¦å‘Šãƒ­ã‚°"""
        self.main_logger.log(BUSINESS_WARNING, message, extra=extra)

    def _system_monitoring(self):
        """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
        while self.monitoring_active:
            try:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')

                self.metrics['system_health'] = {
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory.percent,
                    'memory_used_gb': round(memory.used / (1024**3), 2),
                    'memory_total_gb': round(memory.total / (1024**3), 2),
                    'disk_percent': disk.percent,
                    'disk_used_gb': round(disk.used / (1024**3), 2),
                    'disk_total_gb': round(disk.total / (1024**3), 2),
                    'timestamp': datetime.utcnow().isoformat()
                }

                # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
                if cpu_percent > 80:
                    self.main_logger.warning(f"ğŸ”¥ High CPU usage: {cpu_percent}%")

                if memory.percent > 85:
                    self.main_logger.warning(f"ğŸ§  High memory usage: {memory.percent}%")

                if disk.percent > 90:
                    self.main_logger.warning(f"ğŸ’¾ High disk usage: {disk.percent}%")

                time.sleep(60)  # 1åˆ†é–“éš”ã§ç›£è¦–

            except Exception as e:
                self.main_logger.error(f"System monitoring error: {str(e)}")
                time.sleep(60)

    def get_metrics(self) -> Dict[str, Any]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        metrics = self.metrics.copy()

        # è¿½åŠ æƒ…å ±
        metrics['pipeline_total'] = metrics['pipeline_successes'] + metrics['pipeline_failures']
        metrics['pipeline_success_rate'] = (
            (metrics['pipeline_successes'] / metrics['pipeline_total'] * 100)
            if metrics['pipeline_total'] > 0 else 0
        )
        metrics['error_rate'] = (
            (metrics['errors'] / metrics['requests'] * 100)
            if metrics['requests'] > 0 else 0
        )

        return metrics

    def export_logs(self, hours: int = 24, format: str = 'json') -> str:
        """ãƒ­ã‚°ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        export_data = {
            'export_time': datetime.utcnow().isoformat(),
            'hours_requested': hours,
            'format': format,
            'metrics': self.get_metrics(),
            'note': 'Full log parsing implementation requires additional development'
        }

        if format == 'json':
            return json.dumps(export_data, indent=2, ensure_ascii=False)
        else:
            return str(export_data)

    def shutdown(self):
        """ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†"""
        self.monitoring_active = False
        self.main_logger.info("ğŸ”š BetValue Finder Logging System shutdown")

        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for logger in [self.main_logger, self.pipeline_logger, self.api_logger, self.error_logger]:
            for handler in logger.handlers:
                handler.close()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
log_manager = BetValueLogManager()

def get_log_manager() -> BetValueLogManager:
    """ãƒ­ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’å–å¾—"""
    return log_manager