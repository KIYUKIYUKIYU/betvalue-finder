# -*- coding: utf-8 -*-
"""
NLP Enhanced Parser
æ©Ÿæ¢°å­¦ç¿’ãƒ»è‡ªç„¶è¨€èªå‡¦ç†ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒ™ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿è§£æ
"""

import re
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict

# NLPé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
try:
    import spacy
    from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
    NLP_AVAILABLE = True
except ImportError:
    spacy = None
    pipeline = None
    NLP_AVAILABLE = False

from app.universal_parser import UniversalBetParser


@dataclass
class ParseResult:
    """ãƒ‘ãƒ¼ã‚¹çµæœã¨ä¿¡é ¼åº¦æƒ…å ±"""
    games: List[Dict]
    confidence: float
    method_used: str
    entities_found: List[Dict]
    processing_time: float
    fallback_used: bool = False


@dataclass
class EntityInfo:
    """æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±"""
    text: str
    label: str
    confidence: float
    start_pos: int
    end_pos: int


class LocalNLPParser:
    """ãƒ­ãƒ¼ã‚«ãƒ«NLPæ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾åº¦ãƒ‘ãƒ¼ã‚µãƒ¼"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.fallback_parser = UniversalBetParser()

        # NLP ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.nlp_model = None
        self.ner_pipeline = None
        self.is_nlp_ready = False

        self._init_nlp_models()

        # ãƒãƒ¼ãƒ åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (ã‚¨ãƒ³ãƒãƒ³ã‚¹ç‰ˆ)
        self.team_database = self._build_enhanced_team_database()

        # ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ (å¼·åŒ–ç‰ˆ)
        self.handicap_patterns = self._build_enhanced_handicap_patterns()

        # ä¿¡é ¼åº¦è¨ˆç®—ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.confidence_weights = {
            "entity_quality": 0.4,
            "pattern_match": 0.3,
            "context_coherence": 0.2,
            "data_completeness": 0.1
        }

    def _init_nlp_models(self):
        """NLPãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        if not NLP_AVAILABLE:
            self.logger.warning("NLP libraries not available. Using fallback mode.")
            return

        try:
            # spaCyæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
            try:
                self.nlp_model = spacy.load("ja_core_news_sm")
                self.logger.info("âœ… spaCy Japanese model loaded successfully")
            except OSError:
                self.logger.warning("âš ï¸ spaCy Japanese model not found. Install with: python -m spacy download ja_core_news_sm")
                self.nlp_model = None

            # Transformers NERãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (è»½é‡ç‰ˆ)
            try:
                # æ—¥æœ¬èªBERTè»½é‡ç‰ˆã‚’ä½¿ç”¨
                self.ner_pipeline = pipeline(
                    "ner",
                    model="cl-tohoku/bert-base-japanese-char",
                    aggregation_strategy="simple",
                    device=-1  # CPUä½¿ç”¨
                )
                self.logger.info("âœ… Transformers NER pipeline loaded successfully")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Transformers NER pipeline failed to load: {e}")
                self.ner_pipeline = None

            # ã©ã¡ã‚‰ã‹ä¸€ã¤ã§ã‚‚åˆ©ç”¨å¯èƒ½ãªã‚‰æº–å‚™å®Œäº†
            self.is_nlp_ready = (self.nlp_model is not None) or (self.ner_pipeline is not None)

            if self.is_nlp_ready:
                self.logger.info("ğŸ§  NLP-enhanced parsing ready")
            else:
                self.logger.warning("âš ï¸ No NLP models available, using rule-based fallback")

        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize NLP models: {e}")
            self.is_nlp_ready = False

    def _build_enhanced_team_database(self) -> Dict[str, Dict]:
        """æ‹¡å¼µãƒãƒ¼ãƒ åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        import json
        import os

        team_database = {}

        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        data_dir = os.path.join(os.path.dirname(__file__), "data")

        # èª­ã¿è¾¼ã‚€JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        team_files = [
            "teams_mlb.json",
            "teams_npb.json",
            "teams_premier.json",
            "teams_laliga.json",
            "teams_bundesliga.json",
            "teams_serie_a.json",
            "teams_ligue1.json",
            "teams_eredivisie.json",
            "teams_primeira_liga.json",
            "teams_scottish_premiership.json",
            "teams_jupiler_league.json",
            "teams_champions_league.json",
            "teams_national.json"
        ]

        total_teams = 0
        for file_name in team_files:
            file_path = os.path.join(data_dir, file_name)
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                        team_database.update(file_data)
                        total_teams += len(file_data)
                        self.logger.info(f"âœ… {file_name}: {len(file_data)}ãƒãƒ¼ãƒ èª­ã¿è¾¼ã¿å®Œäº†")
                else:
                    self.logger.warning(f"âš ï¸ Team data file not found: {file_path}")
            except Exception as e:
                self.logger.error(f"âŒ Failed to load {file_name}: {e}")

        self.logger.info(f"ğŸ¯ Total teams loaded: {total_teams} teams")

        # ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not team_database:
            self.logger.warning("âš ï¸ No team data loaded, using minimal fallback")
            team_database = {
                "å·¨äºº": {"sport": "npb", "full_name": "èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„", "aliases": ["ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„", "Giants"]},
                "é˜ªç¥": {"sport": "npb", "full_name": "é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹", "aliases": ["ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹", "Tigers"]},
                "ãƒ‰ã‚¸ãƒ£ãƒ¼ã‚¹": {"sport": "mlb", "full_name": "Los Angeles Dodgers", "aliases": ["LAD", "Dodgers"]},
                "ãƒ¤ãƒ³ã‚­ãƒ¼ã‚¹": {"sport": "mlb", "full_name": "New York Yankees", "aliases": ["NYY", "Yankees"]},
            }

        return team_database

    def _build_enhanced_handicap_patterns(self) -> List[Dict]:
        """å¼·åŒ–ã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        return [
            # NPBå®Ÿéš›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼š<07>, <02>, <0> ãªã©ï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
            {"pattern": r"<(\d+)>", "type": "npb_bracket_handicap", "confidence": 0.99},

            # å°æ•°ç‚¹ä»˜ãæ‹¬å¼§å½¢å¼ï¼š<0.2>, <1.5> ãªã©
            {"pattern": r"<(\d+\.\d+)>", "type": "decimal_bracket_handicap", "confidence": 0.98},

            # åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
            {"pattern": r"([+-]?\d+(?:\.\d+)?)", "type": "decimal", "confidence": 0.9},
            {"pattern": r"(\d+)åŠ", "type": "half_suffix", "confidence": 0.95},
            {"pattern": r"(\d+)\.5", "type": "point_five", "confidence": 0.95},

            # æ—¥æœ¬å¼è¡¨è¨˜
            {"pattern": r"(\d+)/(\d+)", "type": "fraction", "confidence": 0.9},
            {"pattern": r"([+-]?\d+(?:\.\d+)?)ç‚¹", "type": "point_suffix", "confidence": 0.8},

            # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³
            {"pattern": r"([+-])(\d+(?:\.\d+)?)", "type": "signed_number", "confidence": 0.85},
            {"pattern": r"(\d+)ç‚¹åŠ", "type": "point_half", "confidence": 0.9},

            # è‹±èªè¡¨è¨˜
            {"pattern": r"([+-]?\d+(?:\.\d+)?)\\s*(?:points?|pts?)", "type": "english_points", "confidence": 0.7},
        ]

    def parse(self, text: str) -> ParseResult:
        """NLPå¼·åŒ–ãƒ‘ãƒ¼ã‚¹ - ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        import time
        start_time = time.time()

        try:
            if self.is_nlp_ready:
                result = self._parse_with_nlp(text)
            else:
                result = self._parse_with_fallback(text)

            processing_time = time.time() - start_time
            result.processing_time = processing_time

            self.logger.info(f"ğŸ¯ Parse completed: {result.method_used}, confidence: {result.confidence:.2f}, time: {processing_time:.3f}s")
            return result

        except Exception as e:
            self.logger.error(f"âŒ Parse failed: {e}")
            return self._parse_with_fallback(text)

    def _parse_with_nlp(self, text: str) -> ParseResult:
        """NLPæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ¼ã‚¹"""

        # Step 1: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        entities = self._extract_entities(text)

        # Step 2: ãƒãƒ¼ãƒ åè­˜åˆ¥
        teams = self._identify_teams(entities, text)

        # Step 3: ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—æŠ½å‡º
        handicaps = self._extract_handicaps_enhanced(text, entities)

        # Step 4: ã‚²ãƒ¼ãƒ æ§‹ç¯‰
        games = self._build_games_from_entities(teams, handicaps, text)

        # Step 5: ä¿¡é ¼åº¦è¨ˆç®—
        confidence = self._calculate_confidence(entities, teams, handicaps, games)

        # Step 6: ä½ä¿¡é ¼åº¦æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if confidence < 0.6:
            fallback_result = self._parse_with_fallback(text)
            if fallback_result.confidence > confidence:
                fallback_result.fallback_used = True
                return fallback_result

        return ParseResult(
            games=games,
            confidence=confidence,
            method_used="nlp_enhanced",
            entities_found=[entity.__dict__ if hasattr(entity, '__dict__') else entity for entity in entities],
            processing_time=0.0,  # å¾Œã§è¨­å®š
            fallback_used=False
        )

    def _extract_entities(self, text: str) -> List[EntityInfo]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º (spaCy + Transformers)"""
        entities = []

        # spaCy ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        if self.nlp_model:
            doc = self.nlp_model(text)
            for ent in doc.ents:
                entities.append(EntityInfo(
                    text=ent.text,
                    label=ent.label_,
                    confidence=0.8,  # spaCyã¯ä¿¡é ¼åº¦ã‚’æä¾›ã—ãªã„ãŸã‚å›ºå®šå€¤
                    start_pos=ent.start_char,
                    end_pos=ent.end_char
                ))

        # Transformers NERè¿½åŠ 
        if self.ner_pipeline and len(text) < 1000:  # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é¿ã‘ã‚‹
            try:
                ner_results = self.ner_pipeline(text)
                for result in ner_results:
                    entities.append(EntityInfo(
                        text=result['word'],
                        label=result['entity_group'],
                        confidence=result['score'],
                        start_pos=result['start'],
                        end_pos=result['end']
                    ))
            except Exception as e:
                self.logger.warning(f"âš ï¸ Transformers NER failed: {e}")

        return entities

    def _identify_teams(self, entities: List[EntityInfo], text: str) -> List[Dict]:
        """ãƒãƒ¼ãƒ åè­˜åˆ¥ (NLP + ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°)"""
        teams = []

        # NLPã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰ãƒãƒ¼ãƒ å€™è£œæŠ½å‡º
        team_candidates = []
        for entity in entities:
            if entity.label in ['PERSON', 'ORG', 'PER', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG']:
                team_candidates.append(entity.text)

        # è¡Œã”ã¨ã«ãƒãƒ¼ãƒ æ¤œå‡ºï¼ˆä½ç½®æƒ…å ±ä¿æŒï¼‰
        lines = text.split('\n')
        for line_idx, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            for team_name, team_info in self.team_database.items():
                if team_name in line:
                    teams.append({
                        "name": team_name,
                        "full_name": team_info["full_name"],
                        "sport": team_info["sport"],
                        "confidence": 0.9,
                        "method": "exact_match",
                        "line_position": line_idx  # è¡Œä½ç½®ã‚’è¿½åŠ 
                    })
                else:
                    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒãƒƒãƒãƒ³ã‚°ï¼ˆè¡Œã”ã¨ï¼‰
                    for alias in team_info["aliases"]:
                        # çŸ­ã„ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆ3æ–‡å­—ä»¥ä¸‹ï¼‰ã¯å˜èªå¢ƒç•Œã‚’ä½¿ç”¨
                        if len(alias) <= 3:
                            import re
                            pattern = r'\b' + re.escape(alias) + r'\b'
                            if re.search(pattern, line, re.IGNORECASE):
                                teams.append({
                                    "name": team_name,
                                    "full_name": team_info["full_name"],
                                    "sport": team_info["sport"],
                                    "confidence": 0.8,  # ç²¾å¯†ãƒãƒƒãƒãªã®ã§ä¿¡é ¼åº¦å‘ä¸Š
                                    "method": "word_boundary_match",
                                    "line_position": line_idx
                                })
                                break
                        # é•·ã„ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆ4æ–‡å­—ä»¥ä¸Šï¼‰ã¯éƒ¨åˆ†ãƒãƒƒãƒãƒ³ã‚°
                        elif alias in line:
                            teams.append({
                                "name": team_name,
                                "full_name": team_info["full_name"],
                                "sport": team_info["sport"],
                                "confidence": 0.7,
                                "method": "alias_match",
                                "line_position": line_idx  # è¡Œä½ç½®ã‚’è¿½åŠ 
                            })
                            break  # ãƒãƒƒãƒã—ãŸã‚‰ã“ã®ãƒãƒ¼ãƒ ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹æ¢ç´¢ã‚’çµ‚äº†

        # å€™è£œã¨ã®ãƒãƒƒãƒãƒ³ã‚°
        for candidate in team_candidates:
            for team_name, team_info in self.team_database.items():
                if self._fuzzy_match(candidate, team_name) > 0.8:
                    teams.append({
                        "name": team_name,
                        "full_name": team_info["full_name"],
                        "sport": team_info["sport"],
                        "confidence": 0.6,
                        "method": "nlp_fuzzy_match"
                    })

        # é‡è¤‡é™¤å»ãƒ»ã‚½ãƒ¼ãƒˆ
        seen = set()
        unique_teams = []
        for team in sorted(teams, key=lambda x: x["confidence"], reverse=True):
            team_key = team["name"]
            if team_key not in seen:
                seen.add(team_key)
                unique_teams.append(team)

        return unique_teams[:10]  # æœ€å¤§10ãƒãƒ¼ãƒ 

    def _extract_handicaps_enhanced(self, text: str, entities: List[EntityInfo]) -> List[Dict]:
        """å¼·åŒ–ã•ã‚ŒãŸãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—æŠ½å‡ºï¼ˆè¡Œä½ç½®è¿½è·¡ä»˜ãï¼‰"""
        handicaps = []
        lines = text.split('\n')

        # NLPã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰æ•°å€¤æŠ½å‡º
        for entity in entities:
            if entity.label in ['CARDINAL', 'QUANTITY', 'NUM', 'B-NUM', 'I-NUM']:
                try:
                    # æ•°å€¤ã®å‰å¾Œæ–‡è„ˆãƒã‚§ãƒƒã‚¯
                    context = self._get_context(text, entity.start_pos, entity.end_pos)
                    if self._is_handicap_context(context):
                        # è¡Œä½ç½®ã‚’è¨ˆç®—
                        line_position = self._find_line_position(text, entity.start_pos)
                        handicaps.append({
                            "value": entity.text,
                            "confidence": entity.confidence * 0.8,
                            "method": "nlp_entity",
                            "context": context,
                            "line_position": line_position
                        })
                except:
                    pass

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹æŠ½å‡ºï¼ˆè¡Œä½ç½®ä»˜ãï¼‰
        for pattern_info in self.handicap_patterns:
            matches = re.finditer(pattern_info["pattern"], text, re.IGNORECASE)
            for match in matches:
                line_position = self._find_line_position(text, match.start())
                raw_value = match.group(1) if match.groups() else match.group(0)

                # NPBç‰¹æ®Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†: <07> â†’ 0.7, <02> â†’ 0.2
                if pattern_info["type"] == "npb_bracket_handicap" and raw_value.isdigit():
                    if len(raw_value) == 2 and raw_value != "00":
                        # 2æ¡ã®å ´åˆã¯å°æ•°ç‚¹å½¢å¼ã«å¤‰æ›
                        converted_value = f"0.{raw_value[1]}" if raw_value[0] == "0" else f"{raw_value[0]}.{raw_value[1]}"
                    elif raw_value == "0":
                        converted_value = "0"
                    else:
                        converted_value = raw_value
                else:
                    converted_value = raw_value

                handicaps.append({
                    "value": converted_value,
                    "confidence": pattern_info["confidence"],
                    "method": f"pattern_{pattern_info['type']}",
                    "context": self._get_context(text, match.start(), match.end()),
                    "line_position": line_position
                })

        return handicaps

    def _fuzzy_match(self, text1: str, text2: str) -> float:
        """ç°¡æ˜“ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°"""
        if not text1 or not text2:
            return 0.0

        # æ­£è¦åŒ–
        t1 = text1.lower().strip()
        t2 = text2.lower().strip()

        if t1 == t2:
            return 1.0

        # éƒ¨åˆ†ä¸€è‡´
        if t1 in t2 or t2 in t1:
            return 0.8

        # å…±é€šæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹
        common_chars = len(set(t1) & set(t2))
        total_chars = len(set(t1) | set(t2))

        return common_chars / total_chars if total_chars > 0 else 0.0

    def _get_context(self, text: str, start: int, end: int, window: int = 20) -> str:
        """å‘¨è¾ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
        start_pos = max(0, start - window)
        end_pos = min(len(text), end + window)
        return text[start_pos:end_pos]

    def _find_line_position(self, text: str, char_position: int) -> int:
        """æ–‡å­—ä½ç½®ã‹ã‚‰è¡Œç•ªå·ã‚’è¨ˆç®—"""
        lines_before = text[:char_position].count('\n')
        return lines_before

    def _is_handicap_context(self, context: str) -> bool:
        """ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—æ–‡è„ˆåˆ¤å®š"""
        handicap_keywords = ["ãƒãƒ³ãƒ‡", "ãƒãƒ³ãƒ‡ã‚£", "ç‚¹", "ãƒã‚¤ãƒ³ãƒˆ", "å·®", "+", "-", "ç‚¹å·®", "<", ">", "18:00"]
        # æ™‚åˆ»ã¨ä¸€ç·’ã«å‡ºç¾ã™ã‚‹<>ã‚‚æœ‰åŠ¹ã¨ã¿ãªã™
        time_pattern = r"\d{1,2}:\d{2}"
        import re
        if re.search(time_pattern, context) and ("<" in context or ">" in context):
            return True
        return any(keyword in context for keyword in handicap_keywords)

    def _build_games_from_entities(self, teams: List[Dict], handicaps: List[Dict], text: str) -> List[Dict]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‹ã‚‰è©¦åˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        games = []

        if len(teams) >= 2:
            # ãƒšã‚¢ä½œæˆ (ãƒªãƒ¼ã‚°ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰²æ–¹å¼)
            paired_teams = self._create_line_based_pairs(teams, text)

            for team_a, team_b in paired_teams:

                game = {
                    "team_a": team_a["name"],
                    "team_b": team_b["name"],
                    "team_a_confidence": team_a["confidence"],
                    "team_b_confidence": team_b["confidence"],
                    "sport": team_a.get("sport", "unknown")
                }

                # è¡Œä½ç½®ã«åŸºã¥ã„ã¦ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’ãƒãƒƒãƒãƒ³ã‚°
                matched_handicap = self._find_handicap_for_teams(team_a, team_b, handicaps)
                if matched_handicap:
                    game["handicap"] = matched_handicap["value"]
                    game["handicap_confidence"] = matched_handicap["confidence"]

                    # ãƒ•ã‚§ã‚¤ãƒãƒªãƒƒãƒˆåˆ¤å®š (ç°¡æ˜“ç‰ˆ)
                    if "+" in matched_handicap["value"] or "-" in matched_handicap["value"]:
                        if "-" in matched_handicap["value"]:
                            game["fav_team"] = team_a["name"]
                        else:
                            game["fav_team"] = team_b["name"]

                games.append(game)

        return games

    def _create_line_based_pairs(self, teams: List[Dict], text: str) -> List[Tuple[Dict, Dict]]:
        """ãƒªãƒ¼ã‚°åã‚’åŒºåˆ‡ã‚Šã¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã€ãƒãƒ¼ãƒ ã‚’ãƒšã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ (Robust Version)"""
        from collections import defaultdict
        import re

        lines = text.split('\n')
        league_markers = []
        # ãƒªãƒ¼ã‚°ãƒãƒ¼ã‚«ãƒ¼ï¼ˆä¾‹: <ãƒªãƒ¼ã‚¬>ï¼‰ã®ä½ç½®ã‚’ç‰¹å®š
        for i, line in enumerate(lines):
            match = re.match(r'[<>\[\]](.+?)[>\<]', line) # < > ã¨ [ ] ä¸¡æ–¹ã«å¯¾å¿œ
            if match:
                league_markers.append({"name": match.group(1), "line": i})
        
        # ãƒãƒ¼ãƒ ã‚’è¡Œç•ªå·ã«åŸºã¥ã„ã¦ã‚½ãƒ¼ãƒˆã—ã¦ãŠã
        sorted_teams = sorted(teams, key=lambda x: x.get("line_position", 0))

        if not league_markers:
            # ãƒªãƒ¼ã‚°ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯ã€å˜ç´”ã«2ã¤ãšã¤ãƒšã‚¢ã«ã™ã‚‹
            return [tuple(sorted_teams[i:i+2]) for i in range(0, len(sorted_teams), 2)]

        # ãƒãƒ¼ãƒ ã‚’ãƒªãƒ¼ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‹
        blocks = defaultdict(list)
        for team in sorted_teams:
            team_line = team["line_position"]
            assigned_league = "default"
            for marker in reversed(league_markers):
                if team_line > marker["line"]:
                    assigned_league = f"{marker['name']}_{marker['line']}" # åŒã˜ãƒªãƒ¼ã‚°åãŒè¤‡æ•°ã‚ã‚‹å ´åˆã‚‚åŒºåˆ¥
                    break
            blocks[assigned_league].append(team)

        # å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ãƒšã‚¢ãƒªãƒ³ã‚°
        pairs = []
        for league_name, teams_in_block in blocks.items():
            for i in range(0, len(teams_in_block) - 1, 2):
                pairs.append((teams_in_block[i], teams_in_block[i+1]))
                
        return pairs

    def _find_handicap_for_teams(self, team_a: Dict, team_b: Dict, handicaps: List[Dict]) -> Optional[Dict]:
        """ãƒãƒ¼ãƒ ãƒšã‚¢ã®è¡Œä½ç½®ã«åŸºã¥ã„ã¦ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’æ¤œç´¢"""
        if not handicaps:
            return None

        # ãƒãƒ¼ãƒ ã®è¡Œä½ç½®ç¯„å›²ã‚’å–å¾—
        team_a_line = team_a.get("line_position", 0)
        team_b_line = team_b.get("line_position", 0)

        # ãƒãƒ¼ãƒ ãƒšã‚¢ã®è¡Œä½ç½®ç¯„å›²
        min_team_line = min(team_a_line, team_b_line)
        max_team_line = max(team_a_line, team_b_line)

        # åŒã˜ãƒšã‚¢å†…ã®ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’æ¤œç´¢ï¼ˆè¡Œä½ç½®ãŒè¿‘ã„ã‚‚ã®ï¼‰
        candidates = []
        for handicap in handicaps:
            h_line = handicap.get("line_position", 0)
            # ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ãŒãƒãƒ¼ãƒ ãƒšã‚¢ã®è¡Œä½ç½®ç¯„å›²å†…ã¾ãŸã¯è¿‘ã„å ´åˆ
            if min_team_line <= h_line <= max_team_line + 1:
                distance = min(abs(h_line - team_a_line), abs(h_line - team_b_line))
                candidates.append((handicap, distance))

        if candidates:
            # æœ€ã‚‚è¿‘ã„ä½ç½®ã®ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’é¸æŠ
            candidates.sort(key=lambda x: (x[1], -x[0]["confidence"]))
            return candidates[0][0]

        return None

    def _calculate_confidence(self, entities: List[EntityInfo], teams: List[Dict], handicaps: List[Dict], games: List[Dict]) -> float:
        """ç·åˆä¿¡é ¼åº¦è¨ˆç®—"""

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å“è³ªã‚¹ã‚³ã‚¢
        entity_quality = 0.0
        if entities:
            avg_entity_confidence = sum(e.confidence for e in entities) / len(entities)
            entity_quality = avg_entity_confidence

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã‚¹ã‚³ã‚¢
        pattern_match = 0.0
        if teams:
            avg_team_confidence = sum(t["confidence"] for t in teams) / len(teams)
            pattern_match = avg_team_confidence

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        context_coherence = 0.0
        if games:
            # åŒã˜ã‚¹ãƒãƒ¼ãƒ„ã®ãƒãƒ¼ãƒ ãŒãƒšã‚¢ã«ãªã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            sports = [g.get("sport", "unknown") for g in games]
            if len(set(sports)) == 1 and sports[0] != "unknown":
                context_coherence = 0.9
            else:
                context_coherence = 0.5

        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢
        data_completeness = 0.0
        if games:
            complete_games = sum(1 for g in games if "handicap" in g and "team_a" in g and "team_b" in g)
            data_completeness = complete_games / len(games)

        # é‡ã¿ä»˜ãç·åˆã‚¹ã‚³ã‚¢
        total_confidence = (
            entity_quality * self.confidence_weights["entity_quality"] +
            pattern_match * self.confidence_weights["pattern_match"] +
            context_coherence * self.confidence_weights["context_coherence"] +
            data_completeness * self.confidence_weights["data_completeness"]
        )

        return min(total_confidence, 1.0)

    def _parse_with_fallback(self, text: str) -> ParseResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ãƒ¼ã‚µãƒ¼ä½¿ç”¨"""
        self.logger.info("ğŸ”„ Using fallback rule-based parser")

        fallback_games = self.fallback_parser.parse(text)

        return ParseResult(
            games=fallback_games,
            confidence=0.7,  # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®åŸºæº–ä¿¡é ¼åº¦
            method_used="rule_based_fallback",
            entities_found=[],
            processing_time=0.0,
            fallback_used=True
        )


class EnhancedUniversalParser:
    """æ—¢å­˜ãƒ‘ãƒ¼ã‚µãƒ¼ã¨NLPå¼·åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼ã®çµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.nlp_parser = LocalNLPParser()
        self.logger = logging.getLogger(__name__)

    def parse(self, text: str) -> List[Dict]:
        """çµ±åˆãƒ‘ãƒ¼ã‚¹ - å¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹äº’æ›"""
        result = self.nlp_parser.parse(text)

        self.logger.info(f"ğŸ“Š Enhanced parse result: {len(result.games)} games, confidence: {result.confidence:.2f}")

        return result.games

    def parse_detailed(self, text: str) -> ParseResult:
        """è©³ç´°æƒ…å ±ä»˜ããƒ‘ãƒ¼ã‚¹"""
        return self.nlp_parser.parse(text)
