# -*- coding: utf-8 -*-
"""
NLP Enhanced Parser
æ©Ÿæ¢°å­¦ç¿’ãƒ»è‡ªç„¶è¨€èªå‡¦ç†ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒ™ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿è§£æ
"""

import re
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict

# NLPé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«)
try:
    import spacy
    from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
    NLP_AVAILABLE = True
except ImportError:
    spacy = None
    pipeline = None
    NLP_AVAILABLE = False

from app.universal_parser import UniversalBetParser
from converter.unified_handicap_converter import jp_to_pinnacle


@dataclass
class ParseResult:
    """ãƒ‘ãƒ¼ã‚¹çµæœã¨ä¿¡é ¼åº¦æƒ…å ±"""
    games: List[Dict]
    confidence: float
    method_used: str
    entities_found: List[Dict]
    processing_time: float
    fallback_used: bool = False


@dataclass
class EntityInfo:
    """æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æƒ…å ±"""
    text: str
    label: str
    confidence: float
    start_pos: int
    end_pos: int


class LocalNLPParser:
    """ãƒ­ãƒ¼ã‚«ãƒ«NLPæ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾åº¦ãƒ‘ãƒ¼ã‚µãƒ¼"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.team_database = self._build_enhanced_team_database()
        self.fallback_parser = UniversalBetParser(team_database=self.team_database)
        self.nlp_model = None
        self.ner_pipeline = None
        self.is_nlp_ready = False
        self._init_nlp_models()
        self.handicap_patterns = self._build_enhanced_handicap_patterns()
        self.confidence_weights = {
            "entity_quality": 0.4,
            "pattern_match": 0.3,
            "context_coherence": 0.2,
            "data_completeness": 0.1
        }

    def _init_nlp_models(self):
        if not NLP_AVAILABLE:
            self.logger.warning("NLP libraries not available. Using fallback mode.")
            return
        try:
            try:
                self.nlp_model = spacy.load("ja_core_news_sm")
                self.logger.info("âœ… spaCy Japanese model loaded successfully")
            except OSError:
                self.logger.warning("âš ï¸ spaCy Japanese model not found. Install with: python -m spacy download ja_core_news_sm")
                self.nlp_model = None
            try:
                self.ner_pipeline = pipeline(
                    "ner",
                    model="cl-tohoku/bert-base-japanese-char",
                    aggregation_strategy="simple",
                    device=-1
                )
                self.logger.info("âœ… Transformers NER pipeline loaded successfully")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Transformers NER pipeline failed to load: {e}")
                self.ner_pipeline = None
            self.is_nlp_ready = (self.nlp_model is not None) or (self.ner_pipeline is not None)
            if self.is_nlp_ready:
                self.logger.info("ğŸ§  NLP-enhanced parsing ready")
            else:
                self.logger.warning("âš ï¸ No NLP models available, using rule-based fallback")
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize NLP models: {e}")
            self.is_nlp_ready = False

    def _build_enhanced_team_database(self) -> Dict[str, Dict]:
        import json
        import os
        team_database = {}
        data_dir = os.path.join(os.path.dirname(__file__), "data")
        team_files = [
            "teams_mlb.json", "teams_npb.json", "teams_premier.json",
            "teams_laliga.json", "teams_bundesliga.json", "teams_serie_a.json",
            "teams_ligue1.json", "teams_eredivisie.json", "teams_primeira_liga.json",
            "teams_scottish_premiership.json", "teams_jupiler_league.json",
            "teams_champions_league.json", "teams_europa_league.json", "teams_national.json"
        ]
        total_teams = 0
        for file_name in team_files:
            file_path = os.path.join(data_dir, file_name)
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                        team_database.update(file_data)
                        total_teams += len(file_data)
            except Exception as e:
                self.logger.error(f"âŒ Failed to load {file_name}: {e}")
        self.logger.info(f"ğŸ¯ Total teams loaded: {total_teams} teams")
        return team_database

    def _parse_japanese_handicap(self, s: str) -> Optional[float]:
        """
        æ—¥æœ¬å¼ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’Pinnacleæ•°å€¤ã«å¤‰æ›
        jp_to_pinnacle()ã‚’ä½¿ç”¨ã—ã¦æ­£ç¢ºãªå¤‰æ›ã‚’è¡Œã†
        """
        s = s.strip()
        try:
            # unified_handicap_converter ã®æ­£ã—ã„å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            return jp_to_pinnacle(s)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªæ•°å€¤å¤‰æ›
            try:
                return float(s)
            except ValueError:
                return None

    def _build_enhanced_handicap_patterns(self) -> List[Dict]:
        return [
            {"pattern": r"<([^>]+)>", "type": "bracket_handicap", "confidence": 0.99},
            {"pattern": r"([+-]?\d+(?:\.\d+)?)", "type": "decimal", "confidence": 0.9},
        ]

    def parse(self, text: str) -> ParseResult:
        import time
        start_time = time.time()
        try:
            if self.is_nlp_ready:
                result = self._parse_with_nlp(text)
            else:
                result = self._parse_with_fallback(text)
            processing_time = time.time() - start_time
            result.processing_time = processing_time
            self.logger.info(f"ğŸ¯ Parse completed: {result.method_used}, confidence: {result.confidence:.2f}, time: {processing_time:.3f}s")
            return result
        except Exception as e:
            self.logger.error(f"âŒ Parse failed: {e}")
            return self._parse_with_fallback(text)

    def _parse_with_nlp(self, text: str) -> ParseResult:
        entities = self._extract_entities(text)
        teams = self._identify_teams(entities, text)
        handicaps = self._extract_handicaps_enhanced(text, entities)
        games = self._build_games_from_entities(teams, handicaps, text)
        confidence = self._calculate_confidence(entities, teams, handicaps, games)
        if confidence < 0.6:
            fallback_result = self._parse_with_fallback(text)
            if fallback_result.confidence > confidence:
                fallback_result.fallback_used = True
                return fallback_result
        return ParseResult(
            games=games, confidence=confidence, method_used="nlp_enhanced",
            entities_found=[e.__dict__ for e in entities], processing_time=0.0, fallback_used=False
        )

    def _extract_entities(self, text: str) -> List[EntityInfo]:
        entities = []
        if self.nlp_model:
            doc = self.nlp_model(text)
            for ent in doc.ents:
                entities.append(EntityInfo(text=ent.text, label=ent.label_, confidence=0.8, start_pos=ent.start_char, end_pos=ent.end_char))
        if self.ner_pipeline and len(text) < 1000:
            try:
                ner_results = self.ner_pipeline(text)
                for result in ner_results:
                    entities.append(EntityInfo(text=result['word'], label=result['entity_group'], confidence=result['score'], start_pos=result['start'], end_pos=result['end']))
            except Exception as e:
                self.logger.warning(f"âš ï¸ Transformers NER failed: {e}")
        return entities

    def _identify_teams(self, entities: List[EntityInfo], text: str) -> List[Dict]:
        teams = []
        lines = text.split('\n')
        for line_idx, line in enumerate(lines):
            line = line.strip()
            if not line: continue
            for team_name, team_info in self.team_database.items():
                if team_name in line:
                    teams.append({"name": team_name, "full_name": team_info["full_name"], "sport": team_info["sport"], "confidence": 0.9, "method": "exact_match", "line_position": line_idx})
                else:
                    for alias in team_info.get("aliases", []):
                        if len(alias) <= 3:
                            if re.search(r'\b' + re.escape(alias) + r'\b', line, re.IGNORECASE):
                                teams.append({"name": team_name, "full_name": team_info["full_name"], "sport": team_info["sport"], "confidence": 0.8, "method": "word_boundary_match", "line_position": line_idx}); break
                        elif alias in line:
                            teams.append({"name": team_name, "full_name": team_info["full_name"], "sport": team_info["sport"], "confidence": 0.7, "method": "alias_match", "line_position": line_idx}); break
        seen = set()
        unique_teams = []
        for team in sorted(teams, key=lambda x: x["confidence"], reverse=True):
            if team["name"] not in seen:
                seen.add(team["name"])
                unique_teams.append(team)
        return unique_teams

    def _extract_handicaps_enhanced(self, text: str, entities: List[EntityInfo]) -> List[Dict]:
        handicaps = []
        lines = text.split('\n')
        for pattern_info in self.handicap_patterns:
            for i, line in enumerate(lines):
                for match in re.finditer(pattern_info["pattern"], line):
                    raw_value = match.group(1)
                    parsed_value = self._parse_japanese_handicap(raw_value)
                    if parsed_value is not None:
                        handicaps.append({"value": parsed_value, "raw_value": raw_value, "confidence": pattern_info["confidence"], "method": f"pattern_{pattern_info['type']}", "line_position": i})
        return handicaps

    def _build_games_from_entities(self, teams: List[Dict], handicaps: List[Dict], text: str) -> List[Dict]:
        from .enhanced_parser_system import (
            UniversalParserQualitySystem,
            create_enhanced_team_candidate,
            TeamCandidate
        )

        # åŒ…æ‹¬çš„å“è³ªå‘ä¸Šã‚·ã‚¹ãƒ†ãƒ é©ç”¨
        quality_system = UniversalParserQualitySystem()

        # æ—¢å­˜ã®è¾æ›¸å½¢å¼ã‹ã‚‰TeamCandidateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        team_candidates = []
        for team in teams:
            team_candidate = create_enhanced_team_candidate(team)
            team_candidates.append(team_candidate)

        # æ›–æ˜§æ€§è§£æ±º
        resolved_teams = quality_system.resolve_team_ambiguity(team_candidates, text)

        # æœ€é©ãƒšã‚¢ãƒªãƒ³ã‚°
        game_candidates = quality_system.create_optimal_pairs(resolved_teams, text)

        # å“è³ªæ¤œè¨¼
        validated_games = quality_system.validate_game_quality(game_candidates)

        # ãƒ¬ã‚¬ã‚·ãƒ¼å½¢å¼ã«å¤‰æ›
        legacy_games = quality_system.export_to_legacy_format(validated_games)

        # ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—æƒ…å ±ã‚’è¿½åŠ 
        for game in legacy_games:
            # team_aã¨team_bã®æƒ…å ±ã‚’ä½¿ã£ã¦è©²å½“ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’æ¤œç´¢
            for candidate in game_candidates:
                if (candidate.home_team.name == game["team_a"] and
                    candidate.away_team.name == game["team_b"]):

                    # å¯¾å¿œã™ã‚‹ãƒãƒ³ãƒ‡ã‚£ã‚­ãƒ£ãƒƒãƒ—ã‚’æ¤œç´¢
                    matched_handicap = self._find_handicap_for_teams(
                        {"name": candidate.home_team.name, "line_position": candidate.home_team.line_position},
                        {"name": candidate.away_team.name, "line_position": candidate.away_team.line_position},
                        handicaps
                    )

                    if matched_handicap:
                        game["handicap"] = matched_handicap["value"]
                        game["raw_handicap"] = matched_handicap["raw_value"]
                        game["handicap_confidence"] = matched_handicap["confidence"]

                        # ãƒ•ã‚§ã‚¤ãƒãƒªãƒƒãƒˆåˆ¤å®š
                        if matched_handicap["line_position"] == candidate.home_team.line_position:
                            game["fav_team"] = candidate.home_team.name
                        elif matched_handicap["line_position"] == candidate.away_team.line_position:
                            game["fav_team"] = candidate.away_team.name
                    break

        return legacy_games

    def _create_line_based_pairs(self, teams: List[Dict], text: str) -> List[Tuple[Dict, Dict]]:
        """ãƒªãƒ¼ã‚°åã‚’åŒºåˆ‡ã‚Šã¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã€ãƒãƒ¼ãƒ ã‚’ãƒšã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ (Robust Version)"""
        from collections import defaultdict
        import re

        lines = text.split('\n')
        league_markers = []
        # ãƒªãƒ¼ã‚°ãƒãƒ¼ã‚«ãƒ¼ï¼ˆä¾‹: <ãƒªãƒ¼ã‚¬>ï¼‰ã®ä½ç½®ã‚’ç‰¹å®š
        for i, line in enumerate(lines):
            match = re.match(r'[<\[](.+?)[>\ ]', line) # < > ã¨ [ ] ä¸¡æ–¹ã«å¯¾å¿œ
            if match:
                league_markers.append({"name": match.group(1), "line": i})
        
        # ãƒãƒ¼ãƒ ã‚’è¡Œç•ªå·ã«åŸºã¥ã„ã¦ã‚½ãƒ¼ãƒˆã—ã¦ãŠã
        sorted_teams = sorted(teams, key=lambda x: x.get("line_position", 0))

        if not league_markers:
            # ãƒªãƒ¼ã‚°ãƒãƒ¼ã‚«ãƒ¼ãŒãªã„å ´åˆã¯ã€ç©ºè¡Œã«ã‚ˆã‚‹è©¦åˆåŒºåˆ‡ã‚Šã‚’èªè­˜ã—ã¦ãƒšã‚¢ã«ã™ã‚‹
            return self._pair_teams_by_empty_line_separation(sorted_teams, text)

        # ãƒãƒ¼ãƒ ã‚’ãƒªãƒ¼ã‚°ãƒ–ãƒ­ãƒƒã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‹
        blocks = defaultdict(list)
        for team in sorted_teams:
            team_line = team["line_position"]
            assigned_league = "default"
            for marker in reversed(league_markers):
                if team_line > marker["line"]:
                    assigned_league = f"{marker['name']}_{marker['line']}" # åŒã˜ãƒªãƒ¼ã‚°åãŒè¤‡æ•°ã‚ã‚‹å ´åˆã‚‚åŒºåˆ¥
                    break
            blocks[assigned_league].append(team)

        # å„ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ãƒšã‚¢ãƒªãƒ³ã‚°
        pairs = []
        for league_name, teams_in_block in blocks.items():
            for i in range(0, len(teams_in_block) - 1, 2):
                pairs.append((teams_in_block[i], teams_in_block[i+1]))
                
        return pairs

    def _find_handicap_for_teams(self, team_a: Dict, team_b: Dict, handicaps: List[Dict]) -> Optional[Dict]:
        team_a_line = team_a.get("line_position", 0)
        team_b_line = team_b.get("line_position", 0)
        min_team_line = min(team_a_line, team_b_line)
        max_team_line = max(team_a_line, team_b_line)
        candidates = []
        for handicap in handicaps:
            h_line = handicap.get("line_position", 0)
            if min_team_line <= h_line <= max_team_line + 1:
                distance = min(abs(h_line - team_a_line), abs(h_line - team_b_line))
                candidates.append((handicap, distance))
        if candidates:
            candidates.sort(key=lambda x: (x[1], -x[0]["confidence"]))
            return candidates[0][0]
        return None

    def _calculate_confidence(self, entities: List[EntityInfo], teams: List[Dict], handicaps: List[Dict], games: List[Dict]) -> float:
        entity_quality = sum(e.confidence for e in entities) / len(entities) if entities else 0.0
        pattern_match = sum(t["confidence"] for t in teams) / len(teams) if teams else 0.0
        sports = [g.get("sport", "unknown") for g in games]
        context_coherence = 0.9 if len(set(sports)) == 1 and sports and sports[0] != "unknown" else 0.5
        data_completeness = sum(1 for g in games if "handicap" in g) / len(games) if games else 0.0
        return min((
            entity_quality * self.confidence_weights["entity_quality"] +
            pattern_match * self.confidence_weights["pattern_match"] +
            context_coherence * self.confidence_weights["context_coherence"] +
            data_completeness * self.confidence_weights["data_completeness"]
        ), 1.0)

    def _filter_teams_by_sport_consistency(self, teams: List[Dict]) -> List[Dict]:
        """ã‚¹ãƒãƒ¼ãƒ„ã®ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒãƒ¼ãƒ ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if len(teams) <= 2:
            return teams

        # ã‚¹ãƒãƒ¼ãƒ„åˆ¥ã«ãƒãƒ¼ãƒ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        sports_groups = {}
        for team in teams:
            sport = team.get('sport', 'unknown')
            if sport not in sports_groups:
                sports_groups[sport] = []
            sports_groups[sport].append(team)

        # æœ€ã‚‚å¤šã„ã‚¹ãƒãƒ¼ãƒ„ã‚’é¸æŠ
        main_sport = max(sports_groups.keys(), key=lambda s: len(sports_groups[s]))

        # åŒä¸€è¡Œç•ªå·ã§è¤‡æ•°ã‚¹ãƒãƒ¼ãƒ„ã«ãƒãƒƒãƒã—ã¦ã„ã‚‹å ´åˆã€ã‚ˆã‚Šé«˜ã„ä¿¡é ¼åº¦ã‚’é¸æŠ
        filtered_teams = []
        processed_lines = set()

        for team in sorted(teams, key=lambda x: (x.get("line_position", 0), -x.get("confidence", 0))):
            line_pos = team.get("line_position", 0)
            if line_pos not in processed_lines:
                # ã“ã®è¡Œç•ªå·ã§æœ€åˆã®ãƒãƒ¼ãƒ ï¼ˆæœ€é«˜ä¿¡é ¼åº¦ï¼‰ã®ã¿è¿½åŠ 
                if team.get('sport') == main_sport:
                    filtered_teams.append(team)
                    processed_lines.add(line_pos)

        self.logger.info(f"ğŸ¯ Sport consistency filter: {len(teams)} â†’ {len(filtered_teams)} teams (main sport: {main_sport})")
        return filtered_teams

    def _pair_teams_by_empty_line_separation(self, teams: List[Dict], text: str) -> List[Tuple[Dict, Dict]]:
        """ç©ºè¡Œã«ã‚ˆã‚‹è©¦åˆåŒºåˆ‡ã‚Šã‚’èªè­˜ã—ã¦ãƒãƒ¼ãƒ ã‚’ãƒšã‚¢ãƒªãƒ³ã‚°"""
        lines = text.split('\n')

        # ç©ºè¡Œã®ä½ç½®ã‚’ç‰¹å®š
        empty_lines = set(i for i, line in enumerate(lines) if not line.strip())

        # ãƒãƒ¼ãƒ ã‚’è¡Œç•ªå·ã§ã‚½ãƒ¼ãƒˆ
        sorted_teams = sorted(teams, key=lambda x: x.get("line_position", 0))

        if not empty_lines:
            # ç©ºè¡ŒãŒãªã„å ´åˆã¯å˜ç´”ã«2ã¤ãšã¤ãƒšã‚¢ï¼ˆå¥‡æ•°å€‹å¯¾å¿œï¼‰
            pairs = []
            for i in range(0, len(sorted_teams), 2):
                team_pair = sorted_teams[i:i+2]
                if len(team_pair) == 2:
                    pairs.append(tuple(team_pair))
                # å¥‡æ•°å€‹ã®å ´åˆã€æœ€å¾Œã®1ã¤ã¯ã‚¹ã‚­ãƒƒãƒ—
            return pairs

        # è©¦åˆãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆï¼ˆç©ºè¡Œã§åŒºåˆ‡ã‚‰ã‚ŒãŸç¯„å›²å†…ã®ãƒãƒ¼ãƒ ï¼‰
        blocks = []
        current_block = []

        for i, team in enumerate(sorted_teams):
            current_block.append(team)

            # æ¬¡ã®ãƒãƒ¼ãƒ ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if i + 1 < len(sorted_teams):
                current_line = team.get("line_position", 0)
                next_line = sorted_teams[i + 1].get("line_position", 0)

                # ç¾åœ¨ã®ãƒãƒ¼ãƒ ã¨æ¬¡ã®ãƒãƒ¼ãƒ ã®é–“ã«ç©ºè¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                has_empty_between = any(current_line < empty_line < next_line for empty_line in empty_lines)

                if has_empty_between or len(current_block) >= 2:
                    blocks.append(current_block)
                    current_block = []
            else:
                # æœ€å¾Œã®ãƒãƒ¼ãƒ 
                blocks.append(current_block)

        # å„ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰2ãƒãƒ¼ãƒ ãšã¤ãƒšã‚¢ã‚’ä½œæˆ
        pairs = []
        for block in blocks:
            if len(block) >= 2:
                pairs.append((block[0], block[1]))

        return pairs

    def _parse_with_fallback(self, text: str) -> ParseResult:
        self.logger.info("ğŸ”„ Using fallback rule-based parser")
        fallback_games = self.fallback_parser.parse(text)
        return ParseResult(games=fallback_games, confidence=0.7, method_used="rule_based_fallback", entities_found=[], processing_time=0.0, fallback_used=True)

class EnhancedUniversalParser:
    def __init__(self):
        self.nlp_parser = LocalNLPParser()
        self.logger = logging.getLogger(__name__)

    def parse(self, text: str) -> List[Dict]:
        result = self.nlp_parser.parse(text)
        self.logger.info(f"ğŸ“Š Enhanced parse result: {len(result.games)} games, confidence: {result.confidence:.2f}")
        return result.games

    def parse_detailed(self, text: str) -> ParseResult:
        return self.nlp_parser.parse(text)